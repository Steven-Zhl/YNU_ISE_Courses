# Part03 线性模型

# 3.0 Pre

## 3.0.1 目录

* [Part03 线性模型](#part03-线性模型)
* [3.0 Pre](#30-pre)
  * [3.0.1 目录](#301-目录)
* [3.1 基本形式](#31-基本形式)
* [3.2 线性回归](#32-线性回归)
  * [3.2.1 最小二乘的参数估计](#321-最小二乘的参数估计)
    * [(1) 问题分析](#1-问题分析)
    * [(2) 解法优化](#2-解法优化)
  * [3.2.2 对数线性回归](#322-对数线性回归)
* [3.3 对数几率回归](#33-对数几率回归)
  * [3.3.1 单位阶跃函数](#331-单位阶跃函数)
  * [3.3.2 Logistic函数](#332-logistic函数)
  * [3.3.3 解法](#333-解法)
* [3.4 线性判别分析](#34-线性判别分析)
* [3.5 多分类问题](#35-多分类问题)
  * [3.5.1 OvO](#351-ovo)
    * [(1) OvO的思想](#1-ovo的思想)
    * [(2) OvO的优缺点](#2-ovo的优缺点)
  * [3.5.2 OvR](#352-ovr)
    * [(1) OvR的思想](#1-ovr的思想)
    * [(2) OvR的优缺点](#2-ovr的优缺点)
  * [3.5.3 MvM](#353-mvm)
  * [3.5.4 ECOC](#354-ecoc)
* [3.6 类别不平衡问题](#36-类别不平衡问题)

# 3.1 基本形式

* 线性模型的基本形式为$f(x)=\omega_1x_1+\omega_2x_2+...+\omega_dx_d+b$
* 向量形式为$f(x)=\omega^Tx+b$，其中$\omega=(\omega_1,\omega_2,...,\omega_d)^T$

> 可以看出，当$\omega$和$b$确定后，线性模型就确定了，本章接下来的任务就是确定$\omega$和$b$。

# 3.2 线性回归

> 回归：根据已有数据建模，预测未知数据的过程，很明显，确定上述参数做的就是回归这项任务。
>
> 均方误差：[戳这里回顾一下](./Part02-模型评估与选择.md#230-均方误差mean-squared-error-mse)

* 在做线性回归之前，我们要先确定一点：什么样的线性函数是好的？
  * 根据我们朴素的想法：如果这条函数直线经过所有点，那它就是好的。确实，但前提是这得是有可能的。
  * 如果不存在经过所有点的直线，那这条直线至少能够尽量接近所有点，即要求“偏差最小”(这个表述有些不严谨)。
  * 如何去衡量这个偏差呢？首先想到的应该是点到直线的欧氏距离。**广义最小二乘法(GLS)**就是以欧氏距离为偏差，最小化偏差的平方和(即均方误差)。
  * 不过由于点到直线的距离计算起来比较麻烦，大多数时候我们仅使用预测值这一维的距离($f(x_i)-y_i$)作为偏差，这就是**普通最小二乘法(OLS)**。
  * 事实上我们本章只需要知道OLS即可，因为平时也只使用OLS。

## 3.2.1 最小二乘的参数估计

> 这其实就是个优化问题，不过存在闭式解，这里介绍闭式解的推导过程。
>
> 闭式解：通过数学推导，得出的解析解，它可以保证计算结果就是最优解(而梯度下降等方式求出的只是逼近最优解的可行解)。

### (1) 问题分析

* 对于线性回归函数$f(x_i)=\omega x_i+b$，我们的目标是求出最优$\omega^*$和$b^*$，使得$E_{(\omega,b)}=\sum^m_{i=1}(f(x_i)-y_i)^2$最小。这个过程可以视为对于一个函数$E(\omega,b)$求极值的问题，表示为$(\omega^*,b^*)=arg_{(\omega,b)}min\sum^m_{i=1}(y_i-\omega x_i-b)^2$
* >根据高数的老套路，对变量分别求偏导，偏导均为0时，即为极值点。
* 求偏导：
  * $\frac{\partial E_{(\omega,b)}}{\partial \omega}=2(\omega\sum^m_{i=1}x_i^2-\sum^m_{i=1}(y_i-b)x_i)=0$
  * $\frac{\partial E_{(\omega,b)}}{\partial b}=2(mb-\sum^m_{i=1}(y_i-\omega x_i))=0$
* 令上式=0，解方程得：
  * $\omega=\frac{\sum^m_{i=1}y_i(x_i-\bar{x})}{\sum^m_{i=1}x_i^2-\frac{1}{m}(\sum^m_{i=1}x_i)^2}$
  * $b=\frac{1}{m}\sum^m_{i=1}(y_i-\omega x_i)=\bar{y}-\omega\bar{x}$
* 上述结果就是闭式解，这是能正向直接算出来的，而不需要迭代。

### (2) 解法优化

* 上述[(1)](#1-问题分析)中的解法已经能够解决问题了，$x_i$不仅可以是标量，也可以是向量——理论上这就能解决任何线性回归问题了。
* 但仔细想想，上述解法还不够优雅。既然$x_i$都是向量了，为什么还要将$\omega,b$表示为向量+标量的形式呢？对的，解法优化就是将上述运算矩阵化。
* > 设有$m$组样本$x_i$，每个样本$x_i$都是$d$维。
  * 令$\hat{\omega}=(\omega;b)$，拼成列向量，此时$\hat{\omega}$有$(d+1)$维。
  * 令$X$为一个$m\times(d+1)$的矩阵，它是由$x_i$最后增添一个1，然后转置后拼成的。
  * 此时$y$为一个列向量，维度为$m$。
  * 此时该问题表示为$\hat{\omega}^*=arg_{\hat{\omega}}min\sum^m_{i=1}(y-X\hat{\omega})^T(y-X\hat{\omega})$
* 对$\hat{\omega}$求导，得：$\frac{\partial E_{(\hat{\omega})}}{\partial \hat{\omega}}=2X^T(X\hat{\omega}-y)$
* 令上式=0，解方程得：$\hat{\omega}^*=(X^TX)^{-1}X^Ty$
* 这就是最小二乘法的矩阵解法，它仍然是闭式解，理论上能计算出最优解。
* 但很明显，它足够优雅，但不够好用。一方面因为$(X^TX)^{-1}$的计算难度挺大，另一方面它要求$X^TX$满秩，不然有可能解出多个$\hat{\omega}^*$。
* 也就是说，从实用的角度出发，我们需要一个通用、简单的解法，而不一定非得理论上完美，那么这种解法就是迭代类方法，它被广泛用在对数线性回归、逻辑回归等问题中。

## 3.2.2 对数线性回归

> 其实就是y与x不为线性关系，而是指数关系。

* 若$y\propto e^{\omega^Tx+b}$，则可以等价写成$\ln y=\omega^Tx+b$，这样一个函数就是对数线性回归函数。
* 推广来讲，凡是满足$y=g^{-1}(\omega^Tx+b)$的函数，都可称之为“广义线性函数”，其中函数$g(·)$称为联系函数”。

# 3.3 对数几率回归

> 本节我们将讨论线性模型如何解决分类问题
>
> 虽然它叫“对数几率回归”，但实际上它是一个分类问题，而非回归问题。这个断句应该是“对数几率”回归，指几率——这一连续值遵循对数线性模型，但这个几率指的是分类正确的几率，所以究其本质，还是一个分类问题。

* 对于二分类问题，线性模型只需要回答“是”或“否”的问题，因此我们需要将线性模型的输出值转换为“是”或“否”的布尔值。
  * 鉴于多分类问题都可以视为二分类问题的组合，因此我们只讨论二分类问题。

## 3.3.1 单位阶跃函数

* 对于线性回归模型$z=\omega^Tx+b$，最先想到的解决办法是单位阶跃函数：
  * $y=\left\{\begin{matrix}1&z>0;\\0.5&z=0;\\0&z<0;\end{matrix}\right.$
  * 当$z>0$时判断为正例，$z<0$时判断为负例，$z=0$时可任意判别。
  * 但是单位阶跃函数不连续，数学性质不够优秀(比如不可导)，因此不适合用于机器学习。我们期望能找到一个连续可导函数，能将$(-\infty,+\infty)$映射到$(0,1)$，$Logistic$函数就是这样一个函数。

## 3.3.2 Logistic函数

> 也叫“逻辑函数”

* Logistic函数是一个S型函数，它的数学表达式为：$y=\frac{1}{1+e^{-z}}$，它的图像如下：
  * ![Logistic函数](./IMG/3.3.2_Logistic函数.png)
  * 它的优势十分明显：
    * 判别规则十分简单
    * 输出了这一判别的概率
    * 连续可导
* 我们将线性回归函数代入可得$y=\frac{1}{1+e^{-\omega^Tx-b}}$，这就是Logistic回归模型，同样可见，它也是个[对数线性模型](#322-对数线性回归)。
* 进行同样的取$\ln$操作，并整理，得到$\ln\frac{y}{1-y}=\omega^Tx+b$。

## 3.3.3 解法

> 仍然是以0-1二分类问题为例

* 令$\beta=(\omega;b),\hat{x}=(x;1)$，则$\omega^Tx+b$可简写为$\beta^T\hat{x}$。
  * > 这一步的目的是简化表示形式
* 再令$p_1(\hat{x_i};\beta)=p(y=1|\hat{x};\beta)=\frac{e^{\omega^Tx+b}}{1+e^{\omega^Tx+b}}$，$p_0(\hat{x_i};\beta)=p(y=0|\hat{x};\beta)=\frac{1}{1+e^{\omega^Tx+b}}$(其实也就是$1-p_1(\hat{x_i};\beta)$)
  * > 这一步就是把$\beta^T\hat{x}$代入了$Logistic$函数中，注意到此时表达式形式为一条件概率，即在当前$\hat{x},\beta$的条件下，判定$y=1\text{或}y=0$的概率。
* 则似然项可重写为$p(y_i|x_i;\omega_i,b)=y_ip_1(\hat{x_i};\beta)+(1-y_i)p_0(\hat{x};\beta)$
  * > 多观察一下就能发现，此式形式上就是计算期望。因为$y_i$只能取0或1，所以当$y_i=0$时，概率之前取$1-y_i$作为系数。
* 对于上述似然项(期望)，当然是越大说明模型效果越好，所以该问题转化为一个优化问题：$max\ \ell(\omega,b)=\sum\limits^m_{i=1}\ln p(y_i|x_i;\omega,b)$
* 等价于最小化$\ell(\beta)=\sum\limits^m_{i=1}(-y_i\beta^T\hat{x_i}+\ln(1+e^{\beta^T\hat{x_i}}))$，而对于该式，可使用梯度下降法、牛顿法等方法求解最优解。

# 3.4 线性判别分析

> 咕咕咕

# 3.5 多分类问题

> 前面说过，多分类问题都可以视为二分类问题的组合(或者说多分类问题可以拆解为若干个二分类问题)，这节就正是做这个事的。
>
> 本节的3种拆分策略：“一对一”(One vs. One, OvO)、“一对其余”(One vs. Rest, OvR)、“多对多”(Many vs. Many, MvM)。
>
> 本节假定：数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\},y_i\in\{C_1,C_2,\cdots,C_N\}$，即数据集包括了$m$组数据，共有$N$个类别。

## 3.5.1 OvO

<details><summary>突击检查：OVO表示什么意思？</summary>One vs. One，一对一</details>

### (1) OvO的思想

* 将$N$个类别两两组合(握手问题)，组合出共$\frac{N(N-1)}{2}$个二分类器$f_k$。
* 训练时将$C_i$和$C_j$类数据用以训练分类这两类的分类器。
* 测试时将某样本同时提交给所有分类器，最后根据这$\frac{N(N-1)}{2}$个结果，按照投票数最多的类别作为最终判别结果。

### (2) OvO的优缺点

* 优点：训练时间短(每个分类器都是二分类问题，并且只需要和该分类器相关的数据即可)
* 缺点：存储开销和测试时间大($\frac{N(N-1)}{2}$个分类器，且每次测试都要执行这么多次分类)

## 3.5.2 OvR

<details><summary>突击检查：OVR表示什么意思？</summary>One vs. Rest，一对其余</details>

### (1) OvR的思想

* 设计$N$个分类器，每个分类器$f_i$都只做一个问题：当前样本$x_j$是属于$C_i$类还是其他类。
* 测试时将样本提交给每个分类器。
  * 若只有一个分类器预测为正例，则该样本属于该分类器对应的类别。
  * 若有多个分类器预测为正例，则通常考虑各分类器的预测置信度，选择置信度最高的类别作为最终判别结果。

### (2) OvR的优缺点

* 优点：相较于OvO，只训练N个分类器，存储和测试开销小。
* 缺点：训练时间长(训练时要用到全部训练数据)

![OvO和OvR的示例](./IMG/3.5_OvO和OvR的示例.png)

## 3.5.3 MvM

* 那么很明显了，MvM就是每个分类器将判断若干个类作为正类，其余作为负类，但是它的正反类必须有特殊的设计，不能随意选取。这里介绍一下最常用的MvM技术：**输出纠错码**(Error Correcting Output Codes, ECOC)。

## 3.5.4 ECOC

> ECOC的核心步骤为①编码②解码。其过程需要结合MvM的具体过程解释。

* 编码：$M$个分类器，每个分类器都将若干个类作为正类，其余作为负类，这$M$个分类器的判定结果组合起来就是该样本的ECOC码。
* 解码：$M$个分类器分别对测试样本进行预测，每个分类器都会返回一个预测标记，共返回$M$个标记，这$M$个标记就是该样本的ECOC码，通常将其中距离最小的类别作为最终判别结果。
  * > 预测标记通常是+1或-1，这种叫做二元ECOC码；如果预测标记为+1、-1、0，这种叫三元ECOC码。此外距离也可以用欧氏距离或海明距离等来衡量。
* 下面是一个例子：
  * ![ECOC的编码和解码](./IMG/3.5_ECOC的编码和解码.png)
  * 以图(a)为例，有5个分类器，前4行表示这5个分类器$f_i$分别对4个类$C_j$的预测标记。
    * > 多说一句，这个标记可以视为MvM中给每个类分配的一个特征，从这个角度来说，它的思想就和基于CNN的图像分类/手写字识别很像。
  * 第5行，对于一个测试用例，这些分类器仍然能返回一个预测标记。然后好了，拿这个标记与前面各类的标记进行比较，距离最近的类就是最终判别结果。
  * 图(a)中直接给出了测试示例与各类的距离。按照海明距离和欧氏距离，最近的都是$C_3$，所以最终判别结果为$C_3$。而对于(b)来说，海明距离和欧氏距离最近的都是$C_2$，所以最终判别结果为$C_2$。
* ECOC编码的好处在于它对于单个分类器的错误有一定的容忍能力，因为上述计算距离的时候是综合各分类器的差距来计算的，单个错误不一定能影响最终决策。当然，分类器越多，容错能力越强，但开销也越大。

# 3.6 类别不平衡问题

> 咕咕咕
