# Part02-贝叶斯决策理论

> 贝叶斯决策理论的思想是：对给定的某种特征向量$X$，计算它属于各个类别的概率，并认为它属于概率最大的类别。
>
> 这么一看，其实也不算多难

# 2.0 Pre

## 2.0.1 目录

* [Part02-贝叶斯决策理论](#part02-贝叶斯决策理论)
* [2.0 Pre](#20-pre)
  * [2.0.1 目录](#201-目录)
  * [2.0.2 概念汇总](#202-概念汇总)
  * [2.0.3 重点](#203-重点)
  * [2.0.4 基础知识](#204-基础知识)
    * [概率论中的相关基本概念](#概率论中的相关基本概念)
* [2.1 几个重要的概念](#21-几个重要的概念)
  * [2.1.1 先验概率](#211-先验概率)
  * [2.1.2 类(条件)概率密度](#212-类条件概率密度)
  * [2.1.3 后验概率](#213-后验概率)
    * [**贝叶斯公式**](#贝叶斯公式)
* [2.2 几种常用的决策规则](#22-几种常用的决策规则)
  * [2.2.1 基于最小错误率的贝叶斯决策](#221-基于最小错误率的贝叶斯决策)
  * [2.2.2 基于最小风险的贝叶斯决策](#222-基于最小风险的贝叶斯决策)
  * [2.2.3 最大似然比判别规则](#223-最大似然比判别规则)
    * [由最小错误率判别引出最大似然比判别规则](#由最小错误率判别引出最大似然比判别规则)
    * [由最小风险判别引出最大似然比判别规则](#由最小风险判别引出最大似然比判别规则)
    * [后记](#后记)
* [Ques02-例题整理](#ques02-例题整理)
  * [\[计算题·最小错误率的贝叶斯决策\]](#计算题最小错误率的贝叶斯决策)
  * [\[计算题·最小风险的贝叶斯决策\]](#计算题最小风险的贝叶斯决策)

## 2.0.2 概念汇总

## 2.0.3 重点

## 2.0.4 基础知识

### 概率论中的相关基本概念

> 概率分布函数：概率分布函数是概率密度函数在某段区间中的积分值，即$P(a\leq X\leq b)=\int^b_a p(x)dx$，其中，$p(x)$是概率密度函数，$a$和$b$是区间的上下限，它的值表示随机变量X分布在区间$[a,b]$内的概率。
>
> 概率密度函数：概率密度函数可以说是为了方便地理解、表示概率而抽象出的一个概念，单纯的概率密度函数没有意义，它只是概率分布图中的那条曲线的函数。

* 以经典的正态分布为例：
  * 概率密度函数：$p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, -\infty<x<+\infty$
    * ![正态分布概率密度](./IMG/2.0.4_正态分布概率密度.png)
  * 概率分布函数：$P(x)=\frac{1}{\sqrt{2\pi}\sigma}\int^x_{-\infty}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx,-\infty<x<+\infty$
    * ![正态分布概率分布](./IMG/2.0.4_正态分布概率分布.png)
  * 观察上面第二幅图的趋势也能发现，概率分布其实就是概率密度积分。
  * Reference：[正态分布的分布函数和概率密度](https://blog.csdn.net/yyyyypppppzzzzz/article/details/121952196)
* **注意**：通常概率值用$P$表示，概率密度用$p$表示，概率分布函数用$F$表示。

# 2.1 几个重要的概念

> 先验概率、后验概率是意义上的定义，在一个给定的问题中是有明确含义的。
>
> 条件概率/概率密度是形式上的定义，符合$P(B|A)$的形式的就是概率密度，它没有特定的含义。后验概率在形式上就是一种条件概率。
>
> 类比一下，先验/后验概率相当于“库仑力/万有引力”，它有实际含义：两个真空、静止点电荷间的因电荷产生的引力/两个物体间因质量产生的引力；而条件概率/概率密度相当于“大统一场理论”，它尝试从形式上整合包括上述两种力的基本力（虽然还没实现）。

## 2.1.1 先验概率

> 先验概率：在完全不知道其他信息的情况下，通过以往经验知道的概率。
>> 先验概率可以通过抽样、统计甚至是专家经验得到，但通常来说，在一个问题中，先验概率是那些**“我就是知道”的概率**。

## 2.1.2 类(条件)概率密度

> 条件概率密度：在某种确定类别条件下，模式样本X出现的概率密度分布函数。

* 符号表示
  * 概率密度函数：$p(X|\omega_i)(i\in 1,2,...,c)$
  * 条件概率：$P(X|\omega_i)(i\in 1,2,...,c)$，表示在$\omega_i$已经发生的情况下，$X$发生的概率。
  * 在模式识别中，$X$通常指的是样本条件，$\omega_i$指的是某种模式。至于这些指的是什么呢，以后的题目中会慢慢明确的。

## 2.1.3 后验概率

> 后验概率：在某个具体的模式样本X条件下，某种类别出现的概率。
>
> 通常决策时使用的是后验概率。

* 通常描述为“在某个具体的样本$X$条件下，某种类别出现的概率”。
* 后验概率可以用以下[贝叶斯公式](#贝叶斯公式)计算出来：

### **贝叶斯公式**

> 说白了贝叶斯公式就是一个条件概率之间的转换公式，我在上概率论的时候就发现这个了，没什么难的。

* $P(\omega_i | X)=\frac{p(X|\omega_i)P(\omega_i)}{p(X)}$
  * 其中，$p(X)=\sum^c_{i=1}p(X|\omega_i)P(\omega_i)=\sum^c_{i=1}p(X\omega_i)$，$X$发生的概率等于$X$与任一$\omega_i$同时发生的概率之和，这是符合直觉的。
* 或者我们可以用概率论中的公式，会更熟悉一点：$P(A|B)=P(B|A)\times \frac{P(A)}{P(B)}$
* 可以发现贝叶斯公式就是条件概率的转换公式，这两个公式都是这样。

# 2.2 几种常用的决策规则

> **基于最小错误率的贝叶斯决策**和**基于最小风险的贝叶斯决策**

* **先验/后验概率**都是概率值（算出来一个确定的值才能比较大小嘛）
* **类条件概率**既可能是概率值，也可能是概率密度。
* 明确符号：

  |符号|含义|备注|
  |:-:|:-:|:-:|
  |$X$|观测到的样本|或者说“给定的模式”，总之就是要你去判别的东西|
  |$\omega_i$|第$i$个类(模式)|你应当将$X$判别到某一个$\omega_i$中去|
  |$P(\omega_i)$|先验概率|已经知道的一些关于模式的信息|
  |$P(X\|\omega_i)$|类条件概率||
  |$p(X\|\omega_i)$|类条件概率密度|这个是概率密度，不是概率|
  |$P(\omega_i\|X)$|后验概率|你要计算的东西(这个公式的意思就是：已观测到$X$，这个$X$属于$\omega_i$的概率)|
  |$a_i$|判决类别|表示你将$X$判别到$\omega_i$中去|
  |$L(a_i\|\omega_j)$|风险矩阵|表示你将一个$X\in \omega_j$的样本错判到$\omega_i$中所付出的代价，注意，$i=j$时$L(a_i\|\omega_j)$也不一定为0|
  |$R(a_i\|\omega_j)$|条件平均损失|你将一个样本$X$判决到$\omega_i$中，其代价的期望|

## 2.2.1 基于最小错误率的贝叶斯决策
>
> 也称“最大后验概率决策”
>
> 思想：根据贝叶斯公式(倒腾条件概率的公式)、先验概率$P(\omega_i)$和类条件概率$p(X|\omega_i)$,计算后验概率$P(\omega_i|X)$，然后选择后验概率最大的类别作为决策结果。

* 包括二分类和多分类两种情况
  * 二分类：
    $$ \left\{
    \begin{aligned}
    \text{若}P(\omega_1|X) > P(\omega_2|X), & \text{则} & X\in\omega_1\text{类} \\
    \text{若}P(\omega_2|X) > P(\omega_1|X), & \text{则} & X\in\omega_2\text{类}
    \end{aligned}
    \right.$$
  * 多分类：
    * $\text{若}P(\omega_i|X)=max\{P(\omega_j|X)\},j=1,2,3,...,n, \text{则}X\in\omega_i\text{类}$
* 也就是说，需要计算在已知$X$的情况下，每个类别出现的概率，然后选择概率最大的类别作为决策结果。
  * 这是一种很简单，也很符合直觉的决策规则，但它足够好吗？也不一定。
  * 有的时候，决策正确的收益和决策错误的损失不在一个数量级上（比如炒股），这时候就需要结合决策的风险综合考虑了。也就是下一节的[基于最小风险的贝叶斯决策](#222-基于最小风险的贝叶斯决策)

## 2.2.2 基于最小风险的贝叶斯决策

> 它相当于是最小错误率决策的推广，主要改进在于额外考虑了决策错误的损失，我们称之为“风险矩阵”。
>
> 我们处理的思想是这样的：已知观测结果$X$，对于每种判决$a_i$计算其对应的条件平均损失$R(a_i\|X)$，然后选择条件平均损失最小的判决作为决策结果。
>
> 具体计算步骤如下：

* 已知先验概率$P(\omega_i)$和类条件概率$P(X|\omega_i)$，计算后验概率$P(\omega_i|X)$
* 对于每种判决$a_i$，计算其对应的条件平均损失$R(a_i|X)$
  * $R(a_i|X)=\sum_{j=1}^nL(a_i|\omega_j)·P(\omega_j|X)$
    * 也就是：考虑$X$属于每种类别的概率，然后计算每种类别下，将$X$判决到$a_i$类别所付出的代价。
    * 这一步的计算需要知道“风险矩阵”$L(a_i|\omega_j)$，题中也许会给，也许需要自己计算。
  * 计算出所有判决$a_i$的条件平均损失后，选择条件平均损失最小的判决作为决策结果。即认为$X\in \omega_i,R(a_i|X)=min(R(a_j|X))$

## 2.2.3 最大似然比判别规则

> 似然函数：似然是一种表示观测数据($X$)在不同参数下($\omega_i$)的可能性的函数。在贝叶斯决策理论中，类条件概率密度函数$p(X|\omega_i)$就是似然函数。
>> 通常来说，似然函数用于选择参数$\omega_i$，因此似然函数的自变量是$\omega_i$，输出值是$P(X|\omega_i)$，根据最大的输出值选择$\omega_i$作为决策结果。
>>
>> 但是话说回来，我们现在做的所有问题不都是把观测值划分到某个$\omega_i$中吗？似乎完成闭环了....
>
> 似然比函数：两个似然函数的比值：$l_{ij}(x)=\frac{p(X|\omega_i)}{p(X|\omega_j)},i,j=1,2,...c,且i\not ={j}$
>
> 最大似然比判别规则：如果有一个似然函数$p(X|\omega_i)$，它与其他所有似然函数$p(X|\omega_j)$的似然比$l_{ij}$都大于门限$\theta_{ij}$，则认为$X\in\omega_i$。
>> 我们接下来可以发现一个很有意思的事情：前面学的最小错误率决策和最小风险决策，都可以看作是最大似然比判别规则的特例，它们只是$\theta_{ij}$的定义不同。

### 由最小错误率判别引出最大似然比判别规则

> 以[最小错误率的二分类公式](#221-基于最小错误率的贝叶斯决策)为例

1. 假设有$P(\omega_1|X)>P(\omega_2|X)$，则我们认为$X\in\omega_1$
2. 根据贝叶斯公式，上述不等式可化为$\frac{p(X|\omega_1)P(\omega_1)}{P(X)}> \frac{p(X|\omega_2)P(\omega_2)}{P(X)}$
3. 分母消去，得到$p(X|\omega_1)P(\omega_1)>p(X|\omega_2)P(\omega_2)$
4. 此时我们能发现似然比函数的雏形了，两边同时除以$p(X|\omega_2)P(\omega_1)$，得到$l_{12}(X)=\frac{p(X|\omega_1)}{p(X|\omega_2)}>\frac{P(\omega_2)}{P(\omega_1)}$
5. 因此总结出判别门限$\theta_{12}=\frac{P(\omega_2)}{P(\omega_1)}$
6. 也就是说：
   $$ \left\{
    \begin{aligned}
    l_{12}(X)>\theta_{12}, & X\in\omega_1\\
    l_{12}(X)<\theta_{12}, & X\in\omega_2\\
    l_{12}(X)=\theta_{12}, & X\in\omega_1 & \text{或}X\in\omega_2\text{(随你喜好)}\\
    \end{aligned}
    \right.$$

### 由最小风险判别引出最大似然比判别规则

> 同样以二分类为例

1. 假设有$R(a_1=\omega_1|X)<R(a_2=\omega_2|X)$，则我们认为$X\in\omega_1$
2. 在二分类问题中，$R(a_i|X)=\sum_{j=1}^2L(a_i|\omega_j)·P(\omega_j|X)$，所以上式可化为：$[L(a_2|\omega_1)-L(a_1|\omega_1)]P(\omega_1|X)>[L(a_1|\omega_2)-L(a_2|\omega_2)]P(\omega_2|X)$（就是乘开、移项、合并同类项）。
3. 再将上式转化为分式，得：$\frac{P(\omega_1|X)}{P(\omega_2|X)}>\frac{L(a_1|\omega_2)-L(a_2|\omega_2)}{L(a_2|\omega_1)-L(a_1|\omega_1)}$
4. 再结合贝叶斯公式，得到似然比函数：$l_{12}(X)=\frac{P(X|\omega_1)}{P(X|\omega_2)}>\frac{L(a_1|\omega_2)-L(a_2|\omega_2)}{L(a_2|\omega_1)-L(a_1|\omega_1)}·\frac{P(\omega_2)}{P(\omega_1)}$
5. 所以总结出，判别门限$\theta_{12}=\frac{L(a_1|\omega_2)-L(a_2|\omega_2)}{L(a_2|\omega_1)-L(a_1|\omega_1)}·\frac{P(\omega_2)}{P(\omega_1)}$

### 后记

* 那么问题来了，最大似然比判别规则有什么用呢？
* 至少在做题方面，似乎没什么用，因为这个方法明显也不比直接使用前两种来的简单。
* 也许统一起前两个方法就是它的意义吧。

# Ques02-例题整理

## [计算题·最小错误率的贝叶斯决策]

> 题目内容

* 地震预报是比较困难的一个课题，可以根据地震与生物异常反应之间的联系来进行研究
* 根据历史统计数据，地震前一周内出现生物异常反应的概率为50%，而一周内没有发生地震但也出现了生物异常反应的概率为10%。
* 假设一个地区属于地震高发区，发生地震的概率为20%。问：如果某日观察到明显的生物异常反应现象，是否应当渊博一周内将发生地震？

> 分析与解答

1. 确定问题类型：明显是一个二分类问题，不妨设$\omega_1$为地震，$\omega_2$为非地震、$X=1$为观察到生物异常反应，$X=0$为未观察到生物异常反应。
2. 分析已知：已知$X=1$，$P(X=1|\omega_1)=50\%$，$P(X=1|\omega_2)=10\%$，$P(\omega_1)=20\%$，$P(\omega_2)=1-P(\omega_1)=80\%$。
3. 分析所求：由于已观测到$X=1$，因此我们要计算的是$P(\omega_1|X=1)$，即在此基础上地震发生的概率。通常来说如果地震概率大于不地震概率，则应当视为地震。
4. 计算：

   * 摆公式，$P(\omega_1|X=1)=\frac{p(X=1,\omega_1)}{P(X=1)}=\frac{P(X=1|\omega_1)P(\omega_1)}{\sum^2_{i=1}P(X=1|\omega_i)P(\omega_i)}=\frac{0.5\times
   0.2}{0.5\times0.2+0.1\times0.8}=\frac{5}{9}$
   * 因此，应当视为地震，进行报警。

## [计算题·最小风险的贝叶斯决策]

> 题目内容
